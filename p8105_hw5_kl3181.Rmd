---
title: "p8105_hw5_kl3181"
author: Kelley Lou
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Read in the data and clean.
```{r}
homicide_df = 
  read.csv("./homicide_data/homicide-data.csv") %>% 
  mutate(
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    ),
    city_state = str_c(city, state, sep = "_")
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

Create aggregate DF
```{r}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Proportion test for a single city 
```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

Iteration for all cities
```{r}
results_df = 
  aggregate_df %>% 
  mutate(
     prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
     tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
   ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

## Problem 2

Tidy the data!
```{r}
path_df = 
  tibble(
    path = list.files("lda_data")
  ) %>% 
  mutate(
    path = str_c("lda_data/", path),
    data = map(path, read_csv)) %>% 
  unnest(data) %>%
  separate(path, into = c("folder", "arm"), sep = 9) %>%
  separate(arm, into = c("arm", "subject_id"), sep = "_") %>% 
  separate(subject_id, into = c("subject_id", "na"), sep = 2) %>% 
  select(-folder, -na) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week", 
    values_to = "observation",
    names_prefix = "week_"
  ) %>%
  mutate(
    subject_id = str_remove(subject_id, "^0+"),
    week = as.numeric(week),
    subject_id = as.numeric(subject_id)
    )

path_df
```

Making a spaghetti plot
```{r}
path_df %>% 
  group_by(subject_id) %>% 
  ggplot(aes(x = week, y = observation, color = arm)) +
  geom_path() +
  labs(
    title = "Observations on each subject over time",
    x = "Week", 
    y = "Observation"
  )
```
Observations appear to be more similar between the two treatment arms at the beginning of the study, and increase in range of observations as the weeks go by. The experimental group seems to have larger observations which increase over time while the control group seems to stay consistent with quite a bit of variation for each study subject. 


## Problem 3

Simulations!!
Sample from a normal distribution - sample size 30, std 5, change the mean (1 to 6) with 5000 datasets for each different mean. 

Exporting results of hypothesis test to see if mean = 0. (t-test)

See what happens as you move from 0 to 1 and conduct t-test along the way. Start with sim.mean and std function to export t-tests instead

estimate, p-value

plots:
- filter(show results when p value is less than 0.5 and what does the mean look like)
-  
